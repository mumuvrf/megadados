{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Databases\n",
    "\n",
    "Em **NLP** (*Natural Language Processing*), frases e documentos são frequentemente representados como **vetores numéricos**. Isto ocorre, por exemplo, para fornecer integração com *Machine Learning* (modelos de **NLP** frequentemente produzem ou utilizam vetores em suas operações) e pesquisa eficiente por similaridade (ao buscar documentos ou frases semelhantes).\n",
    "\n",
    "Um banco de dados vetorial (*Vector Database*) ou mecanismo de busca vetorial é um **banco de dados** que pode armazenar tais **vetores** (listas de números de comprimento fixo) juntamente com outros itens de dados, enquanto permite recuperar informação de forma **eficiente** e **escalável**.\n",
    "\n",
    "Nesta aula, exploraremos desde a representação vetorial de textos até uma aplicação de **RAG** (*Retrieval-Augmented Generation*) com **LLM** (*Large Language Model*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criar ambiente virtual\n",
    "\n",
    "É extremamente recomendável que você realize esta aula em um novo ambiente virtual.\n",
    "\n",
    "- Criar venv com `conda`\n",
    "\n",
    "```console\n",
    "conda create -n mdvector python=3.10\n",
    "conda activate mdvector\n",
    "```\n",
    "\n",
    "- Criar venv com `python -m`\n",
    "\n",
    "```console\n",
    "python -m venv venv\n",
    "\n",
    "# Ativar no Windows\n",
    "venv\\Scripts\\activate\n",
    "\n",
    "# Ativar no Linux/MacOS\n",
    "source venv/bin/activate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalação das Libs\n",
    "\n",
    "Após criar e ativar seu ambiente virtual, realize a instalação das dependências:\n",
    "\n",
    "**Atenção:** se for realizar a instalação pelo notebook, garanta que o notebook está executando com o ambiente correto!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chave OpenAI\n",
    "\n",
    "Nesta aula, iremos utilizar a API da OpenAI pela Azure.\n",
    "\n",
    "Utilize as credenciais fornecidas pelo professor. Crie um arquivo `.env` a partir do arquivo `.env.example` e faça a configuração das variáveis de ambiente.\n",
    "\n",
    "**ATENÇÃO**: Utilize esta chave com cuidado. Não desperdice recursos, não execute células múltiplas vezes de forma desnecessária e, mais importante, **NÃO VAZE A CHAVE** publicamente nem compartilhe com terceiros!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textos: Representação Vetorial\n",
    "\n",
    "Vetores são representações matemáticas de dados em um espaço de alta dimensão. Nesse espaço, cada dimensão corresponde a uma característica dos dados, com o número de dimensões variando de algumas centenas a dezenas de milhares, dependendo da complexidade dos dados representados.\n",
    "\n",
    "A maneira mais simples de vetorizar textos é utilizando o método **Bag of Words** (**BoW**). Nele, cada texto é representado pela ocorrência (ou frequência) de suas palavras. Ele ignora a ordem das palavras e considera apenas a presença ou ausência delas no documento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por exemplo, vamos supor um dicionário de uma língua com apenas três palavras:\n",
    "\n",
    "```python\n",
    "dic = [\"oi\", \"bom\", \"horrível\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, considere as seguintes frases:\n",
    "\n",
    "```python\n",
    "frases = [\n",
    "    \"oi tudo bom?\",\n",
    "    \"bom dia\",\n",
    "    \"foi horrível\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos fazer uma representação vetorial das frases pelos seguintes vetores:\n",
    "\n",
    "```python\n",
    "vetores = [\n",
    "    [1, 1, 0], # Tem palavra \"oi\", tem \"bom\", não tem \"horrível\"\n",
    "    [0, 1, 0], # Não tem \"oi\", tem \"bom\", não tem \"horrível\"\n",
    "    [0, 0, 1], # Não tem \"oi\", não tem \"bom\", tem \"horrível\"\n",
    "]\n",
    "```\n",
    "\n",
    "Perceba que ignoramos as palavras não pertencentes ao dicionário.\n",
    "\n",
    "Vamos representar isto em código-fonte!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# Frases personalizadas\n",
    "frases = [\n",
    "    \"oi tudo bom?\",\n",
    "    \"bom dia\",\n",
    "    \"foi horrível\",\n",
    "    \"oi, tive um dia bom e horrível ao mesmo tempo\"\n",
    "]\n",
    "\n",
    "# Vetores das frases, considerando o dicionário [\"oi\", \"bom\", \"horrível\"]\n",
    "vetores = np.array([\n",
    "    [1, 1, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1],\n",
    "    [1, 1, 1],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E visualizar um gráfico dos vetores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cores diferentes para cada ponto\n",
    "cores = [\"#636EFA\", \"#EF553B\", \"#00CC96\", \"#666666\"]\n",
    "\n",
    "# Criar uma figura do Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "# Adicionar pontos ao gráfico\n",
    "for vector, color, frase in zip(vetores, cores, frases):\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=[vector[0]],\n",
    "        y=[vector[1]],\n",
    "        z=[vector[2]],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=5, color=color),\n",
    "        text=f\"{frase} {vector}\",\n",
    "        hoverinfo=\"text\"\n",
    "    ))\n",
    "\n",
    "# Definir título e rótulos dos eixos com limites\n",
    "fig.update_layout(\n",
    "    title=\"Vetores de Frases - Gráfico de Pontos 3D\",\n",
    "    scene=dict(\n",
    "        xaxis=dict(title=\"Eixo oi\", range=[0, 2]),\n",
    "        yaxis=dict(title=\"Eixo bom\", range=[0, 2]),\n",
    "        zaxis=dict(title=\"Eixo horrível\", range=[0, 2])\n",
    "    ),\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Mostrar o gráfico\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similaridade entre vetores\n",
    "\n",
    "A similaridade do cosseno mede o cosseno do ângulo entre dois vetores. É uma medida popular em processamento de linguagem natural e recuperação de informações e pode ser calculada como:\n",
    "\n",
    "\n",
    "$$ s(A,B)=\\frac{A⋅B}{∥A∥∥B∥}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja uma definição em python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    cosine_sim = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "    return cosine_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dica**: Também é possível utilizar a função da biblioteca `scipy`:\n",
    "\n",
    "```python\n",
    "from scipy.spatial.distance import cosine\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos calcular a similaridade entre as frases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular a matriz de distância cosseno\n",
    "num_frases = len(frases)\n",
    "matriz_distancia = np.zeros((num_frases, num_frases))\n",
    "\n",
    "for i in range(num_frases):\n",
    "    for j in range(num_frases):\n",
    "        matriz_distancia[i, j] = cosine_similarity(vetores[i], vetores[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E visualizar o mapa de calor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar uma figura do Plotly\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=matriz_distancia,\n",
    "    x=frases,\n",
    "    y=frases,\n",
    "    colorscale=\"YlOrRd\"\n",
    "))\n",
    "\n",
    "# Definir título e rótulos dos eixos\n",
    "fig.update_layout(\n",
    "    title=\"Matriz de Distância Cosseno\",\n",
    "    xaxis_title=\"Frases\",\n",
    "    yaxis_title=\"Frases\"\n",
    ")\n",
    "\n",
    "# Mostrar o gráfico\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício**: A frase `\"bom dia\"` é mais similar à frase `\"oi tudo bom?\"` ou à frase `\"foi horrível\"`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sua resposta aqui!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício**: Qual seria a representação vetorial da frase `\"olá tenham todos um dia horrível\"` considerando o mesmo dicionário do exemplo?\n",
    "\n",
    "<a href=\"#\" title=\"[0, 0, 1] pois apenas a palavra 'horrivel' está no dicionário e no texto!\">Passe o mouse aqui para conferir resposta.</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sua resposta aqui!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indo além do BoW: Word Embeddings\n",
    "\n",
    "Embora seja simples e eficiente, o **BoW** tem várias limitações. Primeiramente, ele não captura a semântica das palavras. Por exemplo, as palavras `\"cão\"` e `\"cachorro\"` são sinônimos, mas no **BoW**, elas são tratadas como palavras completamente diferentes.\n",
    "\n",
    "Além disso, **BoW** também não considera a **ordem das palavras**, o que pode ser problemático para entender o contexto. Por exemplo, as frases `\"o gato mordeu o cachorro\"` e `\"o cachorro mordeu o gato\"` têm significados muito diferentes, mas no **BoW**, elas podem ser representadas da mesma forma se contiverem as mesmas palavras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esses problemas levam à necessidade de representações de texto mais avançadas, que capturam a semântica e o contexto das palavras. É aqui que os *embeddings* entram em cena. *Embeddings* são representações vetoriais densas onde **palavras com significados semelhantes têm representações semelhantes**.\n",
    "\n",
    "<img src=\"img/embedding.png\">\n",
    "\n",
    "\n",
    "Diferentemente do **BoW**, que representa cada palavra como um valor único em um vetor esparso, os *embeddings* mapeiam palavras para um espaço **vetorial contínuo** de relativa baixa dimensão. Isso permite que relações semânticas e contextuais sejam preservadas.\n",
    "\n",
    "Por exemplo, em um espaço de *embeddings*, as palavras `\"rei\"` e `\"rainha\"` estarão próximas uma da outra, refletindo sua semelhança semântica, enquanto estarão longe de uma palavra como `\"carro\"`.\n",
    "\n",
    "<style>\n",
    "    .image-container {\n",
    "        background-color: white;\n",
    "        display: inline-block;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"image-container\">\n",
    "    <img src=\"img/emb_vectors.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os modelos utilizados para *embedding* são geralmente treinados em grandes corpora de texto. Esses modelos aprendem as representações vetoriais das palavras com base em seu contexto de uso nas frases, capturando assim **nuances semânticas** e **sintáticas** que **BoW** não consegue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dica**: os modelos modernos de redes neurais em **NLP** processam **tokens** em vez de palavras. Um token é a menor unidade de texto que pode ser processada pelo modelo. Tokens podem ser palavras, caracteres, sinais de pontuação, símbolos ou partes de palavras.\n",
    "\n",
    "Acesse o link https://platform.openai.com/tokenizer e experimente o tokenizador online da OpenAI. Digite alguns textos, com pontuações, e observe os tokens gerados.\n",
    "\n",
    "<img src=\"img/openai_tokens.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentenceTransformers Embeddings\n",
    "\n",
    "Vamos utilizar a biblioteca [**SentenceTransformers**](https://sbert.net/) para obter o embedding de textos.\n",
    "\n",
    "Veja mais em https://sbert.net/\n",
    "\n",
    "Podemos especificar o modelo com (veja mais modelos em https://www.sbert.net/docs/sentence_transformer/pretrained_models.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model_name = \"multi-qa-MiniLM-L6-cos-v1\"\n",
    "embedding_model = SentenceTransformer(embedding_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E utilizar para fazer o embedding de uma frase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vetor = embedding_model.encode(\"meu cachorro é muito fofo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conferindo o resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"O vetor tem {len(vetor)} dimensões\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vetor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos definir algumas frases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frases_novas = [\n",
    "    \"meu cachorro é muito fofo\",\n",
    "    \"iphone 15 pro max é muito caro\",\n",
    "    \"quero um pet que é amigo das crianças\",\n",
    "    \"smartphones estão focando em IA\",\n",
    "    \"apple encerra programa de carro autônomo\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E utilizar programação funcional para fazer o embedding de cada frase.\n",
    "\n",
    "**Dica**: poderíamos ter utilizado laço `for`, utilizar programação funcional é só um detalhe aqui! Para cada frase, será mapeada a função `embedding_model.embed_query`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vetores_novos = embedding_model.encode(frases_novas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtemos como resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"A matriz possui {len(vetores_novos)} vetores\")\n",
    "print(f\"Cada vetor possui {len(vetores_novos[0])} dimensões\")\n",
    "print(f\"Ou seja, temos uma matriz {len(vetores_novos)}x{len(vetores_novos[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos calcular a similaridade entre as frases pelo cálculo da similaridade cosseno entre os seus vetores:\n",
    "\n",
    "Confira quais frases são mais similares entre si."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "similarity_matrix = cosine_similarity(vetores_novos)\n",
    "\n",
    "fig = px.imshow(similarity_matrix, \n",
    "                x=frases_novas, \n",
    "                y=frases_novas, \n",
    "                color_continuous_scale=\"Viridis\",\n",
    "                labels={\"x\": \"Frases\", \"y\": \"Frases\", \"color\": \"Similaridade\"})\n",
    "fig.update_layout(title=\"Heatmap de Similaridades Cosseno\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Busca\n",
    "\n",
    "Vamos supor que possuímos muitos textos representados de forma vetorial. Os textos poderiam ser parágrafos de diversos livros ou notícias. Se o usuário tiver alguma questão e desejar encontrar os vetores mais semelhantes à sua pergunta. Por exemplo:\n",
    "\n",
    "```python\n",
    "\"Quais são os objetivos da área de tecnologia em 2024?\"\n",
    "```\n",
    "\n",
    "Como isto poderia ser feito?\n",
    "\n",
    "**Exercício**: Como você faria para realizar esta busca? Quais seriam os passos?\n",
    "\n",
    "<a href=\"#\" title=\"1) fazer embedding do texto da pergunta 2) calcular a distância entre o vetor do texto da pergunta e todos os vetores armazenados 3) retornar os mais semelhantes.\">Pare o mouse aqui para conferir resposta.</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sua resposta AQUI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício**: Comparar o vetor da pergunta com todos os vetores é eficiente de um ponto de vista computacional?\n",
    "\n",
    "<a href=\"#\" title=\"Não! Pois seria O(kn), onde k é o tamanho do vetor e n é a quantidade de vetores. Apesar de ser linear no número de vetores, seria ineficiente ao considerar um número elevado de vetores.\">Pare o mouse aqui para conferir resposta.</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sua resposta AQUI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício**: Utilizar um SGBD relacional poderia ajudar? Pense em como um SGBD realiza buscas e qual seria o impacto nesta situação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sua resposta AQUI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChromaDB\n",
    "\n",
    "Da mesma maneira que **SGBDs** como o **MySQL** implementam uma série de recursos para trabalhar com dados segundo o modelo relacional, também temos opções para bancos de dados vetoriais.\n",
    "\n",
    "Nesta aula, iremos utilizar o [**Chroma DB**](https://www.trychroma.com/), mas temos outras opções comumente utilizadas:\n",
    "\n",
    "- Pinecone (cloud)\n",
    "- Milvus\n",
    "- Qdrant\n",
    "- PostgreSQL (um SGBD com módulo pgVector para vector DB)\n",
    "\n",
    "<img src=\"img/chroma.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como exemplo, vamos utilizar o livro da Alice no País das Maravilhas (`data/alice.txt`).\n",
    "\n",
    "Vamos dividir o livro em pedaços ou *chunks*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "chunk_size = 300\n",
    "\n",
    "raw_documents = TextLoader(\"data/alice.txt\").load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0, separator=\"\\n\")\n",
    "documents = text_splitter.split_documents(raw_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja um documento gerado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Então, criamos o banco de dados. Observe que iremos aplicar o `embedding_model`, que irá requerer chamadas à SentenceTransformers. Ainda, será criada uma pasta `alice_chroma_*`.\n",
    "\n",
    "Mas antes, criaremos uma classe para possibilitar que o LangChain consiga trabalhar com a SentenceTransformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.base import Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List\n",
    "\n",
    "class SentenceTransformerEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_documents(self, documents: List[str]) -> List[List[float]]:\n",
    "        return self.model.encode(documents)\n",
    "\n",
    "    def embed_query(self, query: str) -> List[float]:\n",
    "        return self.model.encode([query])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, criamos nossa *VectorStore* com os embeddings e os documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "chroma_embedding_model = SentenceTransformerEmbeddings(model_name=embedding_model_name)\n",
    "data_path = f\"./alice_chroma_{embedding_model_name.lower()}\"\n",
    "\n",
    "db = Chroma.from_documents(\n",
    "    documents, chroma_embedding_model, persist_directory=data_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após executar uma vez, comente a célula acima que cria o `db` (uma vez que ele já existe). Então, vamos passar a abrir direto do arquivo (VectorStore)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "db = Chroma(\n",
    "    embedding_function=chroma_embedding_model, persist_directory=data_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, podemos utilizar o `db` para recuperar textos cujos vetores sejam mais similares à determinada pergunta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pergunta = \"O que a lagarta tirou da boca?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para isto, será necessário calcular o vetor da pergunta (utilizando o mesmo modelo de *embedding* aplicado aos dados)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_pergunta = chroma_embedding_model.embed_query(pergunta)\n",
    "\n",
    "print(f\"O vetor da pergunta tem dimensão {len(embedding_pergunta)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Então, podemos encontrar os `k` vetores mais similares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_resposta = db.similarity_search_by_vector(embedding_pergunta, k=5)\n",
    "\n",
    "for doc in docs_resposta:\n",
    "    # Exibir até 150 primeiros caracteres do conteúdo\n",
    "    print(f\"{doc.page_content[:150]}...\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercício**: teste com outras perguntas! Confira no arquivo `data/alice.txt` se as respostas fazem sentido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sua resposta AQUI!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por enquanto apenas retornamos os textos cujos vetores são mais semelhantes à pergunta. O texto é um recorte do texto original e o usuário precisa ler os textos e procurar a resposta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG\n",
    "\n",
    "**RAG** (**Retrieval Augmented Generation**), é uma técnica que combina **recuperação de informações** com um modelo **LLM** de geração de texto para melhorar os resultados. Este método fornece ao modelo informações que podem estar mais atualizadas do que o conjunto de dados usado no treinamento.\n",
    "\n",
    "Ainda, pode prover informações específicas de um contexto, por exemplo, quando queremos respostas envolvendo apenas os dados de um PDF empresarial privado.\n",
    "\n",
    "A estrutura de uma solução RAG envolve três etapas principais:\n",
    "\n",
    "1) Identificação de documentos relevantes que refletem o contexto da pergunta\n",
    "2) Combinação desse contexto com um prompt que contém instruções específicas\n",
    "3) geração do texto utilizando um LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso prompt será construído com:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Você é um assistente para tarefas de resposta a perguntas. Use as seguintes partes do contexto recuperado para responder à pergunta. Se você não sabe a resposta, basta dizer que não sabe. Use no máximo três frases e mantenha a resposta concisa.\n",
    "\n",
    "Pergunta: {question} \n",
    "\n",
    "Contexto: {context} \n",
    "\n",
    "Resposta:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"question\", \"context\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos especificar onde os textos de contexto são recuperados. Iremos utilizar o `db` (Chroma DB construído anteriormente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como **LLM**, iremos utilizar o modelo `gpt-4.1-nano`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt-4o_MacielVidal_Chave1\",\n",
    "    api_version=\"2025-01-01-preview\",\n",
    "    temperature=0,\n",
    "    max_tokens=1000,\n",
    "    timeout=30,\n",
    "    max_retries=2,\n",
    "    model=\"gpt-4.1-nano\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E criar uma *RAG chain* com:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Então, podemos fazer nossas perguntas, que serão respondidas utilizando o texto da Alice como referência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"Qual o título do livro? E do terceiro capítulo?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"Quem é o coelho?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"O que a lagarta tirou da boca? Com quem a lagarta estava? O que ela disse?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para verificar que de fato o contexto é importante, vamos criar um *retriever* vazio e tentar repetir a pergunta. Perceba que, como não temos os dados do livro da Alice, a API do ChatGPT não irá responder corretamente (faltou contexto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_retriever = Chroma(embedding_function=chroma_embedding_model).as_retriever()\n",
    "\n",
    "rag_chain_no_retriever = (\n",
    "    {\n",
    "        \"context\": empty_retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_no_retriever.invoke(\"O que a lagarta tirou da boca? Com quem a lagarta estava? O que ela disse?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dica:** Os prompts podem ser obtidos direto do *hub* langchain:\n",
    "\n",
    "```python\n",
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "```\n",
    "\n",
    "Veja mais em https://smith.langchain.com/hub/rlm/rag-prompt e https://smith.langchain.com/hub/rlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chainlit\n",
    "\n",
    "Vamos criar um *app* para conversar com o livro da Alice utilizando o *chainlit* https://docs.chainlit.io/.\n",
    "\n",
    "Rode a célula abaixo para criar um arquivo `app.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "\"\"\"Alice no País das Maravilhas - RAG\n",
    "This is a simple RAG (Retrieval-Augmented Generation) application that uses LangChain and\n",
    "Azure OpenAI to answer questions about the book \"Alice in Wonderland\".\n",
    "It uses Chroma as the vector database and SentenceTransformer for embeddings.\n",
    "It is designed to be run in a Chainlit app.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import List\n",
    "import chainlit as cl\n",
    "from chromadb.config import Settings\n",
    "from dotenv import load_dotenv\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "\n",
    "class SentenceTransformerEmbeddings(Embeddings):\n",
    "    \"\"\"Wrapper around SentenceTransformer to use with LangChain.\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed a list of documents.\"\"\"\n",
    "        return self.model.encode(texts)\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Embed a query.\"\"\"\n",
    "        return self.model.encode([text])[0]\n",
    "\n",
    "\n",
    "def singleton(cls):\n",
    "    \"\"\"Decorator to make a class a singleton.\n",
    "    This decorator ensures that a class has only one instance\n",
    "    and provides a global point of access to it.\n",
    "    \"\"\"\n",
    "    instances = {}\n",
    "\n",
    "    def get_instance(*args, **kwargs):\n",
    "        if cls not in instances:\n",
    "            instances[cls] = cls(*args, **kwargs)\n",
    "        return instances[cls]\n",
    "\n",
    "    return get_instance\n",
    "\n",
    "\n",
    "@singleton\n",
    "class MyRAG:\n",
    "    \"\"\"Singleton class for the RAG chain.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        def get_prompt():\n",
    "            \"\"\"Get the prompt template.\"\"\"\n",
    "            prompt_template = \"\"\"\n",
    "Você é um assistente para tarefas de resposta a perguntas. Use as seguintes partes do contexto recuperado para responder à pergunta. Se você não sabe a resposta, basta dizer que não sabe. Use no máximo três frases e mantenha a resposta concisa. Negue qualquer informação que não esteja no contexto recuperado.\n",
    "Pergunta: {question} \n",
    "\n",
    "Contexto: {context} \n",
    "\n",
    "Resposta:\n",
    "\"\"\"\n",
    "\n",
    "            prompt = PromptTemplate(\n",
    "                template=prompt_template, input_variables=[\"question\", \"context\"]\n",
    "            )\n",
    "\n",
    "            return prompt\n",
    "\n",
    "        def get_llm():\n",
    "            \"\"\"Get the LLM. Use Azure OpenAI.\"\"\"\n",
    "            llm = AzureChatOpenAI(\n",
    "                azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "                api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "                temperature=0,\n",
    "                max_tokens=int(os.getenv(\"MAX_TOKENS\", \"1000\")),\n",
    "                timeout=int(os.getenv(\"TIMEOUT\", \"30\")),\n",
    "                max_retries=2,\n",
    "                model=os.getenv(\"LLM_MODEL\"),\n",
    "            )\n",
    "            return llm\n",
    "\n",
    "        def format_docs(docs):\n",
    "            return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "        def get_chroma_settings():\n",
    "            \"\"\"Get Chroma settings. Opt out of telemetry.\"\"\"\n",
    "            return Settings(anonymized_telemetry=False)\n",
    "\n",
    "        def get_retriever():\n",
    "            embedding_model_name = os.getenv(\"EMBEDDING_MODEL\")\n",
    "            embedding_model = SentenceTransformerEmbeddings(\n",
    "                model_name=embedding_model_name\n",
    "            )\n",
    "\n",
    "            num_vectors = int(os.getenv(\"NUM_VECTORS\"))\n",
    "\n",
    "            vector_db_dir = os.getenv(\"VECTOR_DB_DIR\")\n",
    "\n",
    "            retriever = Chroma(\n",
    "                embedding_function=embedding_model,\n",
    "                persist_directory=vector_db_dir,\n",
    "                client_settings=get_chroma_settings(),\n",
    "            ).as_retriever(num_vectors=num_vectors)\n",
    "\n",
    "            return retriever\n",
    "\n",
    "        self.rag_chain = (\n",
    "            {\n",
    "                \"context\": get_retriever() | format_docs,\n",
    "                \"question\": RunnablePassthrough(),\n",
    "            }\n",
    "            | get_prompt()\n",
    "            | get_llm()\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "    def invoke(self, pergunta):\n",
    "        \"\"\"Invoke the RAG chain with the given question.\"\"\"\n",
    "        try:\n",
    "            return self.rag_chain.invoke(pergunta)\n",
    "        except Exception as e:\n",
    "            return f\"Erro: {e}\"\n",
    "\n",
    "\n",
    "@cl.on_chat_start\n",
    "async def on_chat_start():\n",
    "    \"\"\"Send a welcome message when the chat starts.\"\"\"\n",
    "    await cl.Message(\n",
    "        content=\"\"\"Oi!\n",
    "Sou a assistente virtual da Alice no País das Maravilhas!\n",
    "Me faça perguntas sobre o livro!\"\"\"\n",
    "    ).send()\n",
    "\n",
    "\n",
    "@cl.on_message\n",
    "async def main(message: cl.Message):\n",
    "    \"\"\"Handle incoming messages.\"\"\"\n",
    "    resp = MyRAG().invoke(message.content)\n",
    "    await cl.Message(content=resp).send()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preencha o arquivo `.env` a partir do `.env.example` (faça uma cópia).\n",
    "\n",
    "Então, no terminal, inicialize a aplicação chainlit com:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```console\n",
    "chainlit run app.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acesse, em seu navegador, a URL fornecida, provavelmente http://localhost:8000\n",
    "\n",
    "Pronto, você criou uma aplicação RAG!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referências\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Vector_database\n",
    "- https://platform.openai.com/docs/guides/embeddings\n",
    "- https://platform.openai.com/tokenizer\n",
    "- https://python.langchain.com/v0.1/docs/use_cases/question_answering/sources/\n",
    "- https://semaphoreci.com/blog/word-embeddings\n",
    "- https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/\n",
    "- Imagens:\n",
    "    - https://docs.trychroma.com/img/hrm4.svg\n",
    "    - https://storage.googleapis.com/gweb-cloudblog-publish/images/image4_fUvNRO7.max-800x800.png\n",
    "- Livro: http://www.ebooksbrasil.org/eLibris/alicep.html autorizado para uso didático conforme http://www.ebooksbrasil.org/\n",
    "- https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.azure.AzureOpenAI.html\n",
    "- https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.azure.AzureChatOpenAI.html\n",
    "- https://sbert.net/\n",
    "- https://www.sbert.net/docs/sentence_transformer/pretrained_models.html\n",
    "- https://huggingface.co/sentence-transformers\n",
    "- https://github.com/langchain-ai/langchain/discussions/7818"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdvector2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
